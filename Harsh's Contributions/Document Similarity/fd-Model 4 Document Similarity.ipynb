{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1] It is seen as a subset of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\\n\\nA subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[4][5] In its application across business problems, machine learning is also referred to as predictive analytics.']\n"
     ]
    }
   ],
   "source": [
    "file_docs = []\n",
    "\n",
    "with open ('sample1.txt') as f:\n",
    "    tokens = f.read()\n",
    "    file_docs.append(tokens)\n",
    "\n",
    "print(file_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=file_docs\n",
    "search_terms = 'machine learning'\n",
    "# search_terms = 'tomato'\n",
    "# search_terms = 'sewing machine'\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "vectors = vectorizer.fit_transform([search_terms] + documents)\n",
    "\n",
    "# Calculate the word frequency, and calculate the cosine similarity of the search terms to the documents\n",
    "cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
    "document_scores = [item.item() for item in cosine_similarities[1:]]  # convert back to native Python dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5842832127691175,)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top-scoring results and their titles\n",
    "scores = [(score) for score in zip(document_scores)]\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tryout 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    \"\"\"\n",
    "    Interface to the WordNet lemmatizer from nltk\n",
    "    \"\"\"\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'wa', 'raining', 'cat', 'and', 'dog', 'in', 'FooBar']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=LemmaTokenizer()\n",
    "\n",
    "tokenizer('It was raining cats and dogs in FooBar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1] It is seen as a subset of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\\n\\nA subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[4][5] In its application across business problems, machine learning is also referred to as predictive analytics.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('sample1.txt') as f:\n",
    "    tokens=f.read()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.557067649429456"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=[tokens]\n",
    "search_terms = 'machine learning is a study that improves automatically such as email filtering'\n",
    "# search_terms = 'sewing machine'\n",
    "\n",
    "# Initialise TfidfVectorizer with the LemmaTokenizer. Also need to lemmatize the stop words as well\n",
    "token_stop = tokenizer(' '.join(stop_words))\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
    "\n",
    "# Calculate the word frequency, and calculate the cosine similarity of the search terms to the documents\n",
    "vectors = vectorizer.fit_transform([search_terms] + documents)\n",
    "cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
    "\n",
    "document_scores = [item.item() for item in cosine_similarities[1:]]  # convert back to native Python dtypes\n",
    "\n",
    "scores = [(score) for score in zip(document_scores)]\n",
    "perc_scores=scores[0][0]*100\n",
    "perc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning is a study that improves automatically'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms = ['machine learning is a study that improves automatically','machine learning study email']\n",
    "search_terms[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Corupus : 36.56 %\n",
      "2  Corupus : 36.56 %\n",
      "3  Corupus : 52.42 %\n",
      "4  Corupus : 100.0 %\n"
     ]
    }
   ],
   "source": [
    "search_terms = ['machine learning is a study that improves automatically such as email filtering','machine learning is a study that improves automatically such as email filtering','machine learning','Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1] It is seen as a subset of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\\n\\nA subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[4][5] In its application across business problems, machine learning is also referred to as predictive analytics.']\n",
    "# search_terms = 'sewing machine'\n",
    "\n",
    "# Initialise TfidfVectorizer with the LemmaTokenizer. Also need to lemmatize the stop words as well\n",
    "token_stop = tokenizer(' '.join(stop_words))\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
    "\n",
    "# Calculate the word frequency, and calculate the cosine similarity of the search terms to the documents\n",
    "\n",
    "for i in range(len(search_terms)):\n",
    "    vectors = vectorizer.fit_transform([search_terms[i]] + documents)\n",
    "    cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
    "\n",
    "    document_scores = [item.item() for item in cosine_similarities[1:]]  # convert back to native Python dtypes\n",
    "\n",
    "    scores = [(score) for score in zip(document_scores)]\n",
    "    perc_scores=scores[0][0]*100\n",
    "    print(i+1,\" Corupus :\",round(perc_scores,2),\"%\")\n",
    "    \n",
    "#perc_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "actual='sample1.txt'\n",
    "query=['machine learning is a study that improves automatically such as email filtering','machine learning is a study that improves automatically such as email filtering','machine learning','Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1] It is seen as a subset of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\\n\\nA subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[4][5] In its application across business problems, machine learning is also referred to as predictive analytics.']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_similarity_score(actual,query):\n",
    "    class LemmaTokenizer:\n",
    "        \n",
    "   \n",
    "        ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "\n",
    "    tokenizer=LemmaTokenizer()\n",
    "    with open (actual) as f:\n",
    "        tokens=f.read()\n",
    "    #tokenizer(tokens)\n",
    "    \n",
    "    documents=tokens\n",
    "    search_terms = query\n",
    "    # search_terms = 'sewing machine'\n",
    "\n",
    "    # Initialise TfidfVectorizer with the LemmaTokenizer. Also need to lemmatize the stop words as well\n",
    "    token_stop = tokenizer(' '.join(stop_words))\n",
    "    vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
    "\n",
    "    # Calculate the word frequency, and calculate the cosine similarity of the search terms to the documents\n",
    "\n",
    "    for i in range(len(search_terms)):\n",
    "        \n",
    "        vectors = vectorizer.fit_transform([search_terms[i]] + [documents])\n",
    "        cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
    "\n",
    "        document_scores = [item.item() for item in cosine_similarities[1:]]  # convert back to native Python dtypes\n",
    "\n",
    "        scores = [(score) for score in zip(document_scores)]\n",
    "        perc_scores=scores[0][0]*100\n",
    "        print(i+1,\" Corupus :\",round(perc_scores,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Corupus : 36.56 %\n",
      "2  Corupus : 36.56 %\n",
      "3  Corupus : 52.42 %\n",
      "4  Corupus : 100.0 %\n"
     ]
    }
   ],
   "source": [
    "get_similarity_score(actual,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts_Tryouts.extract_text_from_pdf import get_text_ocr_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_path=r\"D:\\Hackoff Mod\\Machine Learning.pdf\"\n",
    "\n",
    "query_path=r\"D:\\Hackoff Mod\\Deep Learning.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning   Machine Learning is the field of study that gives computers the capability to learn without being explicitly programmed. ML is one of the most exciting technologies that one would have ever come across. As it is evident from the name, it gives the computer that makes it more similar to humans: The ability to learn. Machine learning is actively being used today, perhaps in many more places than one would expect. Getting started with Machine Learning Last Updated: 11-05-2020 This article discusses the categories of machine learning problems, and terminologies used in the field of machine learning. Types of machine learning problems There are various ways to classify machine learning problems. Here, we discuss the most obvious ones. 1. On basis of the nature of the learning “signal” or “feedback” available to a learning system •  Supervised learning: The computer is presented with example inputs and their desired outputs, given by a “teacher”, and the goal is to learn a general rule that maps inputs to outputs. The training process continues until the model achieves the desired level of accuracy on the training data. Some real-life examples are: •  Image Classification: You train with images/labels. Then in the future you give a new image expecting that the computer will recognize the new object. •  Market Prediction/Regression: You train the computer with historical market data and ask the computer to predict the new price in the future. •  Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. It is used for clustering population in different groups. Unsupervised learning can be a goal in itself (discovering hidden patterns in data). •  Clustering: You ask the computer to separate similar data into clusters, this is essential in research and science. •  High Dimension Visualization: Use the computer to help us visualize high dimension data. •  Generative Models: After a model captures the probability distribution of your input data, it will be able to generate more data. This can be very useful to make your classifier more robust. A simple diagram which clears the concept of supervised and unsupervised learning is shown below: As you can see clearly, the data in supervised learning is labelled, where as data in unsupervised learning is unlabelled. •  Semi-supervised learning: Problems where you have a large amount of input data and only some of the data is labeled, are called semi-supervised learning problems. These problems sit in between both supervised and '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act=get_text_ocr_pdf(actual_path)\n",
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[1][2][3] Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, machine vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[4][5][6] Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[7][8][9] The adjective \"deep\" in deep learning comes from the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, and then that a network with a nonpolynomial activation function with one hidden layer of unbounded width can on the other hand so be. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the \"structured\" part.  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que=get_text_ocr_pdf(query_path)\n",
    "que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_similarity_score(actual,query):\n",
    "    class LemmaTokenizer:\n",
    "        \n",
    "   \n",
    "        ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "\n",
    "    tokenizer=LemmaTokenizer()\n",
    "    \n",
    "    tokens=str(actual)\n",
    "    #tokenizer(tokens)\n",
    "    \n",
    "    documents=tokens\n",
    "    search_terms = str(query)\n",
    "    # search_terms = 'sewing machine'\n",
    "\n",
    "    # Initialise TfidfVectorizer with the LemmaTokenizer. Also need to lemmatize the stop words as well\n",
    "    token_stop = tokenizer(' '.join(stop_words))\n",
    "    vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
    "\n",
    "    # Calculate the word frequency, and calculate the cosine similarity of the search terms to the documents\n",
    "\n",
    "    \n",
    "        \n",
    "    vectors = vectorizer.fit_transform([search_terms] + [documents])\n",
    "    cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
    "    document_scores = [item.item() for item in cosine_similarities[1:]]  # convert back to native Python dtypes\n",
    "\n",
    "    scores = [(score) for score in zip(document_scores)]\n",
    "    perc_scores=scores[0][0]*100\n",
    "    print(\" Corupus Similarity Ratio:\",round(perc_scores,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Corupus Similarity Ratio: 16.71 %\n"
     ]
    }
   ],
   "source": [
    "get_similarity_score(act,que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quer=que+'machine learning algorithms functions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Corupus Similarity Ratio: 18.61 %\n"
     ]
    }
   ],
   "source": [
    "get_similarity_score(act,quer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
